{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertConfig, BertForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import wandb\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import nlp_utils as nu\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "wandb.login()\n",
    "train_data_path = '../data/Subtask_1_train.json'\n",
    "test_data_path = '../data/Subtask_1_test.json'\n",
    "pickle_save_path = '../data/pickle/'\n",
    "model_save_path = '../models/'\n",
    "dataset = pd.read_json(train_data_path)\n",
    "# train_df\n",
    "dataset\n",
    "conversations_df = pd.read_json(train_data_path)['conversation']\n",
    "conversations_df[0]\n",
    "unique_labels = set()\n",
    "unique_speakers = set()\n",
    "\n",
    "for conv in conversations_df:\n",
    "    for utterance in conv:\n",
    "        unique_labels.add(utterance['emotion'])\n",
    "        unique_speakers.add(utterance['speaker'])\n",
    "unique_labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(list(unique_labels))\n",
    "\n",
    "label_dict_save_path = pickle_save_path + \"label_dict.pkl\"\n",
    "labels_dict = {x:label_encoder.transform([x])[0] for x in label_encoder.classes_}\n",
    "pickle.dump(labels_dict, open(label_dict_save_path, \"wb\"))\n",
    "all_conversations = []\n",
    "\n",
    "for conv in conversations_df:\n",
    "    dialog = []\n",
    "    for sentence in conv:\n",
    "        speaker = sentence['speaker']\n",
    "        utterance = nu.preprocess_text(sentence['text'])\n",
    "        dialog.append(utterance)\n",
    "    all_conversations.append(dialog)\n",
    "start_token = '<CLS>'\n",
    "sep_token = '<SEP>'\n",
    "\"\".find\n",
    "dataset\n",
    "all_conversations\n",
    "def find_subsentence_indices(full_sentence, sub_sentence):\n",
    "    start_index = full_sentence.find(sub_sentence)\n",
    "    end_index = start_index + len(sub_sentence) - 1\n",
    "    print(start_index, end_index, full_sentence[start_index:end_index+1])\n",
    "    return start_index, end_index\n",
    "\n",
    "def finding(dataset):\n",
    "    # full_liso = []\n",
    "    liso = []\n",
    "    for i in dataset.values:\n",
    "        id = i[0]\n",
    "        emo_cau_pairs =  i[2]\n",
    "        convo = i[1]\n",
    "        liso.append({\"convo\" : id,\n",
    "                     \"pairs\" : []})\n",
    "        for j in emo_cau_pairs:\n",
    "            a1 = j[0]\n",
    "            a2 = j[1]\n",
    "            checker1 = a1.split(\"_\")[0]\n",
    "            checker2 = a2.split(\"_\")[0]\n",
    "            num1 = int(checker1)\n",
    "            num2 = int(checker2)\n",
    "            s1 = a1[len(checker1)+1:]\n",
    "            s2 = a2[len(checker2)+1:]\n",
    "            for kk in convo:\n",
    "                if(kk[\"utterance_ID\"])==num2:\n",
    "                    # print(kk['text'], s2)\n",
    "                    a,b = find_subsentence_indices(nu.preprocess_text(kk[\"text\"]), nu.preprocess_text(s2))\n",
    "                    \n",
    "                    liso[len(liso)-1][\"pairs\"].append([num1, num2, a, b])\n",
    "\n",
    "    return liso\n",
    "\n",
    "liso=finding(dataset)\n",
    "liso[529]\n",
    "all_conversations[0][0][85:113]\n",
    "all_conversations[529][0][36:74+100]\n",
    "liso\n",
    "cause_labels = []\n",
    "for conv in liso:\n",
    "    map = {}\n",
    "    for arr in conv['pairs']:\n",
    "        if not map.get(arr[0], None):\n",
    "            map[arr[0]] = []\n",
    "        map[arr[0]].append(arr[1])\n",
    "    cause_labels.append(map)\n",
    "def is_cause(target, utt, conv_id):\n",
    "    if utt in cause_labels[conv_id].get(target, []):\n",
    "        return 1\n",
    "    return 0\n",
    "all_tc_pairs = []\n",
    "all_tc_labels = []\n",
    "\n",
    "for conv_id, conv in enumerate(all_conversations):\n",
    "    for target_id, target in enumerate(conv):\n",
    "        for utt_id, utt in enumerate(conv):\n",
    "            all_tc_pairs.append([target, utt])\n",
    "            all_tc_labels.append(is_cause(target_id+1, utt_id+1, conv_id))\n",
    "\n",
    "all_tc_pairs = np.array([np.array(x) for x in all_tc_pairs], dtype='object')\n",
    "all_tc_labels = np.array(all_tc_labels)\n",
    "\n",
    "all_tc_pairs\n",
    "print(all_tc_labels[:20])\n",
    "train_tc_pairs, dev_tc_pairs, train_tc_labels, dev_tc_labels = train_test_split(all_tc_pairs, all_tc_labels, test_size=0.2, random_state=42)\n",
    "# train_conversations = []\n",
    "# dev_conversations = []\n",
    "\n",
    "# train_utterances = []\n",
    "# train_emotions = []\n",
    "# train_speakers = []\n",
    "\n",
    "# dev_utterances = []\n",
    "# dev_emotions = []\n",
    "# dev_speakers = []\n",
    "\n",
    "# for conv in train_conv_df:\n",
    "#     dialog = []\n",
    "#     dialog_emotions = []\n",
    "#     for sentence in conv:\n",
    "#         speaker = sentence['speaker']\n",
    "#         utterance = f'{speaker}: ' + nu.preprocess_text(sentence['text'])\n",
    "#         emotion = sentence['emotion']\n",
    "#         train_utterances.append(utterance)\n",
    "#         dialog.append(utterance)\n",
    "#         train_speakers.append(speaker)\n",
    "#         train_emotions.append(sentence['emotion'])\n",
    "#         dialog_emotions.append(sentence['emotion'])\n",
    "#     train_conversations.append(dialog)\n",
    "\n",
    "# for conv in dev_conv_df:\n",
    "#     dialog = []\n",
    "#     dialog_emotions = []\n",
    "#     for sentence in conv:\n",
    "#         speaker = sentence['speaker']\n",
    "#         utterance = f'{speaker}: ' + nu.preprocess_text(sentence['text'])\n",
    "#         emotion = sentence['emotion']\n",
    "#         dev_utterances.append(utterance)\n",
    "#         dialog.append(utterance)\n",
    "#         dev_speakers.append(speaker)\n",
    "#         dev_emotions.append(sentence['emotion'])\n",
    "#         dialog_emotions.append(sentence['emotion'])\n",
    "#     dev_conversations.append(dialog)\n",
    "\n",
    "# train_emotions = label_encoder.transform(train_emotions)\n",
    "# dev_emotions = label_encoder.transform(dev_emotions)\n",
    "def encode_plus(tc_pairs, tokenizer, max_length=256):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for pair in tqdm(tc_pairs, total=len(tc_pairs), desc=\"Encoding utterances\"):\n",
    "        encoded = tokenizer.encode_plus(pair[0], pair[1],\n",
    "                                add_special_tokens=True,\n",
    "                                max_length=max_length,\n",
    "                                padding='max_length',\n",
    "                                truncation=False,\n",
    "                                return_attention_mask=True,\n",
    "                                return_tensors='pt')\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_masks}\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_contexts_encodings = encode_plus(train_tc_pairs, tokenizer)\n",
    "dev_contexts_encodings = encode_plus(dev_tc_pairs, tokenizer)\n",
    "train_contexts_encodings['input_ids'].shape\n",
    "X_train_context_input_ids = train_contexts_encodings['input_ids'].to(device)\n",
    "X_train_context_attention_masks = train_contexts_encodings['attention_mask'].to(device)\n",
    "\n",
    "X_dev_context_input_ids = dev_contexts_encodings['input_ids'].to(device)\n",
    "X_dev_context_attention_masks = dev_contexts_encodings['attention_mask'].to(device)\n",
    "\n",
    "X_train_context_input_ids.shape\n",
    "# X_dev_context_input_ids.shape\n",
    "train_labels = torch.tensor(train_tc_labels, dtype=torch.long).to(device)\n",
    "dev_labels = torch.tensor(dev_tc_labels, dtype=torch.long).to(device)\n",
    "train_tc_labels.shape\n",
    "torch.save(X_train_context_input_ids, pickle_save_path + 'X_train_tc_input_ids.pt')\n",
    "torch.save(X_train_context_attention_masks, pickle_save_path + 'X_train_tc_attention_masks.pt')\n",
    "torch.save(train_labels, pickle_save_path + 'train_labels_tc.pt')\n",
    "\n",
    "torch.save(X_dev_context_input_ids, pickle_save_path + 'X_dev_tc_input_ids.pt')\n",
    "torch.save(X_dev_context_attention_masks, pickle_save_path + 'X_dev_tc_attention_masks.pt')\n",
    "torch.save(dev_labels, pickle_save_path + 'dev_labels_tc.pt')\n",
    "X_train_context_input_ids = torch.load(pickle_save_path + 'X_train_tc_input_ids.pt')\n",
    "X_train_context_attention_masks = torch.load(pickle_save_path + 'X_train_tc_attention_masks.pt')\n",
    "train_labels = torch.load(pickle_save_path + 'train_labels_tc.pt')\n",
    "\n",
    "X_dev_context_input_ids = torch.load(pickle_save_path + 'X_dev_tc_input_ids.pt')\n",
    "X_dev_context_attention_masks = torch.load(pickle_save_path + 'X_dev_tc_attention_masks.pt')\n",
    "dev_labels = torch.load(pickle_save_path + 'dev_labels_tc.pt')\n",
    "config = BertConfig.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "train_contexts_encodings['input_ids'].shape\n",
    "train_dataset = TensorDataset(X_train_context_input_ids, X_train_context_attention_masks, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "dev_dataset = TensorDataset(X_dev_context_input_ids, X_dev_context_attention_masks, dev_labels)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=16, shuffle=True)\n",
    "def train_step(model, input_ids, attention_mask, labels):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    loss = predictions[0]\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"NLP_Project\",\n",
    "    group='CRC',\n",
    "    name='Bert-crc-tc-run1',\n",
    "\n",
    "    config={\n",
    "        \"architecture\": \"bert\",\n",
    "        'context': 'target-utterance',\n",
    "        'input shape': 'flattened',\n",
    "        \"epochs\": 3,\n",
    "        \"learning_rate\": 1e-5\n",
    "    }\n",
    ")\n",
    "config = wandb.config\n",
    "\n",
    "num_epochs = config.epochs   # Number of epochs, adjust as needed\n",
    "train_losses = []\n",
    "dev_losses = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}')\n",
    "    epoch_train_loss = 0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids.to(device)\n",
    "        attention_mask.to(device)\n",
    "        labels.to(device)\n",
    "        loss = train_step(model, input_ids, attention_mask, labels)\n",
    "        epoch_train_loss += loss\n",
    "    epoch_train_loss /= len(train_loader)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_dev_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dev_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids.to(device)\n",
    "            attention_mask.to(device)\n",
    "            labels.to(device)\n",
    "            predictions = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            dev_loss = predictions[0]\n",
    "            total_dev_loss += dev_loss.item()\n",
    "    avg_dev_loss = total_dev_loss / len(dev_loader)\n",
    "    dev_losses.append(avg_dev_loss)\n",
    "    \n",
    "    wandb.log({'train_loss': epoch_train_loss, 'val_loss': avg_dev_loss})\n",
    "    print(f'Training Loss: {epoch_train_loss}, Validation Loss: {avg_dev_loss}')\n",
    "model.save_pretrained(model_save_path + 'crc-bert-m1')\n",
    "# tokenizer.save_pretrained(model_save_path)\n",
    "plt.figure(figsize=(8, 4))\n",
    "# plt.plot(range(1, num_epochs+1), train_losses, label='Training Loss')\n",
    "# plt.plot(range(1, num_epochs+1), dev_losses, label='Validation Loss')\n",
    "plt.plot(range(1, num_epochs), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs), dev_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dev_loader, desc=f\"Eval Minibatch\"):\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        input_ids.to(device)\n",
    "        attention_mask.to(device)\n",
    "        labels.to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        predictions.append(np.argmax(outputs.logits.to('cpu').numpy(), axis=1))\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "np.array(true_labels)\n",
    "(predictions == 1).any()\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "f1 = f1_score(true_labels, predictions, average='weighted')\n",
    "precision = precision_score(true_labels, predictions, average='weighted')\n",
    "recall = recall_score(true_labels, predictions, average='weighted')\n",
    "print(classification_report(true_labels, predictions))\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
